import streamlit as st
import streamlit as st
from llama_index import VectorStoreIndex, ServiceContext, Document
from llama_index.llms import OpenAI
import openai
from llama_index import SimpleDirectoryReader
from elasticsearch import Elasticsearch
import pandas as pd
from konlpy.tag import *
from konlpy.tag import Okt

openai.api_key = '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'

# elastic setting
cloud_id = 'Demo_v4:YXAtbm9ydGhlYXN0LTIuYXdzLmVsYXN0aWMtY2xvdWQuY29tJGI2OTlmYzE2NWM1NzQ0YmQ4MjBjYWU5NTM0MTM3YjFiJGQ1YzIzNjViNTNjMTQwYzk5YjUyMGE2YTZkNWE2ZGQ5'
username = 'luck4'
password = '000000'
index_name='ottogi_review_v1'


es = Elasticsearch(
    cloud_id=cloud_id,
    basic_auth=(username, password),
)



### ---- langchain ---- ###


st.set_page_config(page_title="ğŸ’¬Chat with the Food" , page_icon="ğŸ’¬", layout="centered", initial_sidebar_state="auto", menu_items=None)
# openai.api_key = st.secrets.openai_key
## hide
# openai.api_key = st.secrets["openapi_key"]
##

st.title("Chat with the Ottogi ğŸ’¬")

if "messages" not in st.session_state.keys(): # Initialize the chat messages history
    st.session_state.messages = [
        {"role": "assistant", "content": "Ask me a question about Ottogi"}
    ]



@st.cache_resource(show_spinner=False)
def load_data():
    with st.spinner(text="Loading our Data â€“ hang tight! This should take 1-2 minutes."):
        
        reader = SimpleDirectoryReader(input_dir="./data", recursive=True)
        docs = reader.load_data()
        service_context = ServiceContext.from_defaults(llm=OpenAI(model="gpt-3.5-turbo", temperature=0.5, system_prompt="You are an expert on ottogi companies food products and reviews"))
        index = VectorStoreIndex.from_documents(docs, service_context=service_context)
        return index
        
if prompt := st.chat_input("Your question"): # Prompt for user input and save to chat history
    
    
    ###----- gpt ----###
    conversation = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": prompt},
        {"role": "assistant", "content": ""}
    ]
    
    # ëŒ€í™”ë¥¼ í†µí•œ ëª¨ë¸ ìš”ì²­
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=conversation
    )
    
    # ëª¨ë¸ ì‘ë‹µ ì¶œë ¥
    gpt_result = response.choices[0].message['content']
    gpt_result = gpt_result.replace(",", " ")    
    print(gpt_result)



    ### ----- langchain ---- ###
    query = {
      "query": {
        "query_string": {
          "query": f"{prompt}*"
        }
      }
    }

    
    response = es.search(index=index_name, body=query, size =50)
    
    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ë³€í™˜
    search_results = []
    for hit in response['hits']['hits']:
        # print(hit)
        search_results.append(hit['_source']['review'])
        
    print(search_results)
    print("length", len(search_results))
        
    with open('./data/output.txt', 'r', encoding='utf-8') as file:
        original_content = file.read()
    
    # íŒŒì¼ì„ ì“°ê¸° ëª¨ë“œë¡œ ì—½ë‹ˆë‹¤.
    with open("./data/output.txt", "w", encoding="utf-8") as file:
        # ê° ë¬¸ìì—´ì„ íŒŒì¼ì— ì‘ì„±í•©ë‹ˆë‹¤.
        for line in search_results:
            file.write(line + "\n")  # ê° ì¤„ ëì— ê°œí–‰ ë¬¸ì("\n")ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    
    print("í…ìŠ¤íŠ¸ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    index = load_data()
    chat_engine = index.as_chat_engine(chat_mode="condense_question", verbose=True) 
    

    st.session_state.messages.append({"role": "user", "content": prompt})

for message in st.session_state.messages: # Display the prior chat messages
    with st.chat_message(message["role"]):
        st.write(message["content"])

# If last message is not from assistant, generate a new response
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            
            response = chat_engine.chat(prompt)
            st.write(f"{response.response} \n\n {gpt_result}")
            message = {"role": "assistant", "content": response.response}
            st.session_state.messages.append(message) # Add response to message history
            
            
            
